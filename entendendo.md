## Tokenization

Tokenization é o processo de transformar uma sequencia de caracteres em um array de tokens que fazem sentido

"""
input: let nums = [A, 20, 30]

output : LET SYMBOL EQ BRACKET SYMBOL ...
"""

## Abstract Syntax Tree 
Vamos construir a AST processando os tokens que recebemos do lexer. O processo de construir um AST dos tokens é chamado de Parsing

## Objetivos

- construir um tokenizer